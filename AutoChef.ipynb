{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AutoChef",
   "id": "7ae4fb54b8ab7ea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare environment and process data",
   "id": "a223744fd8578d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Source: https://www.kaggle.com/datasets/irkaal/foodcom-recipes-and-reviews?resource=download\n",
    "\n",
    "Data Source v2: https://app.roboflow.com/bens-workspace-3xdyh/fridge-detection-aymme/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true"
   ],
   "id": "9cab27aba6e3d9f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T00:35:17.800322Z",
     "start_time": "2025-08-05T00:35:09.248186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import re\n",
    "import PIL.Image\n",
    "from ultralytics import YOLO\n",
    "import torch"
   ],
   "id": "a42200c6c729d180",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Temp\\ipykernel_7020\\3912826509.py\", line 1, in <module>\n",
      "    from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2276, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2304, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\image_transforms.py\", line 22, in <module>\n",
      "    from .image_utils import (\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\transformers\\image_utils.py\", line 59, in <module>\n",
      "    from torchvision.transforms import InterpolationMode\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\__init__.py\", line 6, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\models\\__init__.py\", line 2, in <module>\n",
      "    from .convnext import *\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py\", line 8, in <module>\n",
      "    from ..ops.misc import Conv2dNormActivation, Permute\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\ops\\__init__.py\", line 23, in <module>\n",
      "    from .poolers import MultiScaleRoIAlign\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\ops\\poolers.py\", line 10, in <module>\n",
      "    from .roi_align import roi_align\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torchvision\\ops\\roi_align.py\", line 4, in <module>\n",
      "    import torch._dynamo\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"C:\\Users\\bps78\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install torch torchvision torchaudio",
   "id": "732c16d1419bf0af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"irkaal/foodcom-recipes-and-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "4d63b7772e167246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "recipes = pd.read_csv(path + \"/recipes.csv\")",
   "id": "bcc4e60ccd6f6c71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(recipes.shape)\n",
    "display(recipes.head())\n",
    "print(recipes.columns)"
   ],
   "id": "bcb452ef96c5af1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "#Convert R-style vector strings to Python lists for 'RecipeIngredientParts' column\n",
    "def r_vector_to_list(s):\n",
    "    # Remove c( and )\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'^c\\(|\\)$', '', s)\n",
    "    # Split by comma, strip quotes and whitespace\n",
    "    return [item.strip().strip('\"').strip(\"'\") for item in s.split(',')]\n",
    "\n",
    "recipes['RecipeIngredientParts'] = recipes['RecipeIngredientParts'].apply(r_vector_to_list)"
   ],
   "id": "e08edaeb266e07cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Core functionality",
   "id": "57e2aa0bc08b6c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_ingredients = recipes['RecipeIngredientParts'].explode().unique().tolist()\n",
    "print(len(all_ingredients))"
   ],
   "id": "7eab20641e839c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use CLIP to match images to ingredients",
   "id": "bb3a6ac4e7fac64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_image = PIL.Image.open(\"fridge_test.jpg\")\n",
    "\n",
    "batch_size = 100\n",
    "ingredient_scores = []\n",
    "\n",
    "for i in range(0, len(all_ingredients), batch_size):\n",
    "    batch_ingredients = all_ingredients[i:i + batch_size]\n",
    "    inputs = processor(text=batch_ingredients, images=test_image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    scores = outputs.logits_per_image[0].detach().cpu().numpy()\n",
    "    ingredient_scores.extend(zip(batch_ingredients, scores))"
   ],
   "id": "cb3410b5493045d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the ingredients present in the image, sorted by score\n",
    "ingredient_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "top_ingredients = [(ingredient, score) for ingredient, score in ingredient_scores if score > 20]\n",
    "print(\"Top ingredients in the image:\")\n",
    "for ingredient, score in top_ingredients:\n",
    "    print(f\"{ingredient}: {score:.4f}\")\n",
    "\n"
   ],
   "id": "e8e143c3e064dbd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the dataset to find recipes that match a set of ingredients",
   "id": "f47ba9dc8c208ed4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Take 2 - use YOLO for a simplified approach",
   "id": "a014be852acc2435"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T00:32:44.399613Z",
     "start_time": "2025-08-05T00:32:44.260770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ],
   "id": "1a9d09664c981b4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce GTX 1660\n",
      "2.3.1+cu118\n",
      "11.8\n",
      "Torch: 2.3.1+cu118\n",
      "Torchvision: 0.18.1+cu118\n",
      "CUDA available: True\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T00:36:06.566364Z",
     "start_time": "2025-08-05T00:35:20.026869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.train(data=\"Fridge detection.v1i.yolov8/data.yaml\", epochs=50, imgsz=640)"
   ],
   "id": "3087eb584fe900b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.174  Python-3.11.9 torch-2.3.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660, 6144MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=Fridge detection.v1i.yolov8/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\bps78\\Documents\\GitHub\\shot-tracer\\runs\\detect\\train9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=30\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    757162  ultralytics.nn.modules.head.Detect           [30, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,016,698 parameters, 3,016,682 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING \u001B[34m\u001B[1mAMP: \u001B[0mchecks failed . AMP training on NVIDIA GeForce GTX 1660 GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access  (ping: 0.10.0 ms, read: 586.7202.2 MB/s, size: 117.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\Fridge detection.v1i.yolov8\\train\\labels.cache... 2232 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2232/2232 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access  (ping: 0.10.1 ms, read: 178.728.8 MB/s, size: 34.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\Fridge detection.v1i.yolov8\\valid\\labels.cache... 103 images, 0 backgrounds, 0 corrupt: 100%|██████████| 103/103 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\bps78\\Documents\\GitHub\\shot-tracer\\runs\\detect\\train9\\labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000294, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001B[1mC:\\Users\\bps78\\Documents\\GitHub\\shot-tracer\\runs\\detect\\train9\u001B[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/140 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 379, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 202, in __call__\n    data = t(data)\n           ^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 2192, in __call__\n    labels[\"img\"] = self._format_img(img)\n                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 2243, in _format_img\n    img = torch.from_numpy(img)\n          ^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Numpy is not available\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m model = YOLO(\u001B[33m\"\u001B[39m\u001B[33myolov8n.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mFridge detection.v1i.yolov8/data.yaml\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m640\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:799\u001B[39m, in \u001B[36mModel.train\u001B[39m\u001B[34m(self, trainer, **kwargs)\u001B[39m\n\u001B[32m    796\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[38;5;28mself\u001B[39m.trainer.model\n\u001B[32m    798\u001B[39m \u001B[38;5;28mself\u001B[39m.trainer.hub_session = \u001B[38;5;28mself\u001B[39m.session  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m799\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001B[39m, in \u001B[36mBaseTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    224\u001B[39m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:388\u001B[39m, in \u001B[36mBaseTrainer._do_train\u001B[39m\u001B[34m(self, world_size)\u001B[39m\n\u001B[32m    386\u001B[39m     pbar = TQDM(\u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.train_loader), total=nb)\n\u001B[32m    387\u001B[39m \u001B[38;5;28mself\u001B[39m.tloss = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_callbacks\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mon_train_batch_start\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Warmup\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\build.py:68\u001B[39m, in \u001B[36mInfiniteDataLoader.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001B[39;00m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)):\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    629\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    630\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m631\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    633\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    634\u001B[39m         \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    635\u001B[39m         \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1344\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1345\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._task_info[idx]\n\u001B[32m-> \u001B[39m\u001B[32m1346\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._process_data\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m   1370\u001B[39m \u001B[38;5;28mself\u001B[39m._try_put_index()\n\u001B[32m   1371\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[32m-> \u001B[39m\u001B[32m1372\u001B[39m     \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\_utils.py:705\u001B[39m, in \u001B[36mExceptionWrapper.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    701\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m    702\u001B[39m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[32m    703\u001B[39m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[31mRuntimeError\u001B[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 379, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 202, in __call__\n    data = t(data)\n           ^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 2192, in __call__\n    labels[\"img\"] = self._format_img(img)\n                    ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\bps78\\Documents\\GitHub\\AutoDJ\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 2243, in _format_img\n    img = torch.from_numpy(img)\n          ^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Numpy is not available\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
